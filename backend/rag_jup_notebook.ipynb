{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5748d276-afea-4ade-9053-72566757530f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain.schema import Document\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from bs4 import BeautifulSoup\n",
    "import tiktoken\n",
    "import requests\n",
    "\n",
    "def chunkIt(html_content, max_size=400, overlap=100):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # remove media and cosmetics\n",
    "    for tag in soup.find_all(['img', 'video', 'audio', 'picture', 'iframe', 'object', 'embed', 'source', 'path']):\n",
    "        tag.decompose()\n",
    "    for tag in soup.find_all(['script', 'style', 'noscript']):\n",
    "        tag.decompose()\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk_elements = []\n",
    "    current_chunk_size = 0\n",
    "\n",
    "\n",
    "    ###############################################\n",
    "    def get_top_level_elements(soup):\n",
    "        body = soup.find('body')\n",
    "        if body:\n",
    "            return [child for child in body.children if hasattr(child, 'name')]\n",
    "        else:\n",
    "            return [child for child in soup.children if hasattr(child, 'name')]\n",
    "\n",
    "    def create_chunk_from_elements(elements):\n",
    "        if not elements:\n",
    "            return \"\"\n",
    "        \n",
    "        wrapper = BeautifulSoup('<div></div>', 'html.parser').div\n",
    "        for elem in elements:\n",
    "            wrapper.append(elem.__copy__())\n",
    "        return str(wrapper)\n",
    "    ###############################################\n",
    "\n",
    "    \n",
    "    top_elements = get_top_level_elements(soup)\n",
    "    \n",
    "    if not top_elements:\n",
    "        return [str(soup)]\n",
    "    \n",
    "    for element in top_elements:\n",
    "        element_html = str(element)\n",
    "        element_size = len(element_html)\n",
    "        \n",
    "        if element_size > max_size:\n",
    "            if current_chunk_elements:\n",
    "                chunk_html = create_chunk_from_elements(current_chunk_elements)\n",
    "                chunks.append(chunk_html)\n",
    "                current_chunk_elements = []\n",
    "                current_chunk_size = 0\n",
    "\n",
    "            \n",
    "            chunks.append(element_html)\n",
    "            continue\n",
    "\n",
    "        \n",
    "        if current_chunk_size + element_size > max_size and current_chunk_elements:\n",
    "            chunk_html = create_chunk_from_elements(current_chunk_elements)\n",
    "            chunks.append(chunk_html)\n",
    "\n",
    "            if overlap > 0 and current_chunk_elements:\n",
    "                current_chunk_elements = [current_chunk_elements[-1], element]\n",
    "                current_chunk_size = len(str(current_chunk_elements[-2])) + element_size\n",
    "            else:\n",
    "                current_chunk_elements = [element]\n",
    "                current_chunk_size = element_size\n",
    "        else:\n",
    "            current_chunk_elements.append(element)\n",
    "            current_chunk_size += element_size\n",
    "\n",
    "    \n",
    "    if current_chunk_elements:\n",
    "        chunk_html = create_chunk_from_elements(current_chunk_elements)\n",
    "        chunks.append(chunk_html)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# store both the plain text representation and the html structured chunks.\n",
    "# search on the plain text and work with html documents.\n",
    "class DualRepresentationVectorDB:\n",
    "    def __init__(self, p_d='./chroma'):\n",
    "        self.embeddings = OllamaEmbeddings(\n",
    "            base_url=\"http://localhost:11434\",\n",
    "            model=\"mxbai-embed-large\"\n",
    "        )\n",
    "        self.vectorstore = None\n",
    "        self.persist_directory = p_d\n",
    "\n",
    "    def extract_plain_text(self, html_chunk):\n",
    "        soup = BeautifulSoup(html_chunk, 'html.parser')\n",
    "        return soup.get_text(separator=' ', strip=True)\n",
    "    \n",
    "    def extract_plain_text_with_key_attrs(self, html_chunk):\n",
    "        soup = BeautifulSoup(html_chunk, \"html.parser\")\n",
    "        texts = []\n",
    "    \n",
    "        for text in soup.get_text(separator=\" \", strip=True).split():\n",
    "            texts.append(text)\n",
    "    \n",
    "        # Important attributes, maybe helpful while searching.\n",
    "        key_attrs = [\"title\", \"alt\", \"onclick\", \"aria-label\", \"data-tooltip\"]\n",
    "        for tag in soup.find_all(attrs={attr: True for attr in key_attrs}):\n",
    "            for attr in key_attrs:\n",
    "                if tag.has_attr(attr):\n",
    "                    val = tag[attr]\n",
    "                    if isinstance(val, list):\n",
    "                        texts.extend(val)\n",
    "                    else:\n",
    "                        texts.append(str(val).strip())\n",
    "    \n",
    "        return \" \".join(texts)\n",
    "    \n",
    "    def add_chunks(self, html_chunks, metadata_list=None):\n",
    "        if metadata_list is None:\n",
    "            metadata_list = [{\"chunk_index\": i} for i in range(len(html_chunks))]\n",
    "        \n",
    "        documents = []\n",
    "        for i, (html_chunk, meta) in enumerate(zip(html_chunks, metadata_list)):\n",
    "            plain_text = self.extract_plain_text(html_chunk)\n",
    "            # Store HTML in metadata\n",
    "            meta['original_html'] = html_chunk  \n",
    "            \n",
    "            documents.append(Document(\n",
    "                page_content=plain_text,\n",
    "                metadata=meta\n",
    "            ))\n",
    "        \n",
    "        if self.vectorstore is None:\n",
    "            self.vectorstore = Chroma.from_documents(\n",
    "                documents, self.embeddings, persist_directory=self.persist_directory\n",
    "            )\n",
    "        else:\n",
    "            self.vectorstore.add_documents(documents)\n",
    "    \n",
    "    def search_with_html_context(self, query, k=5):\n",
    "        if not self.vectorstore:\n",
    "            return []\n",
    "            \n",
    "        q_emb = self.embeddings.embed_query(query)\n",
    "        \n",
    "        results = self.vectorstore._collection.query(\n",
    "            query_embeddings=[q_emb],\n",
    "            n_results=k,\n",
    "            include=[\"documents\",\"metadatas\"]\n",
    "        )\n",
    "        docs, metas = results[\"documents\"][0], results[\"metadatas\"][0]\n",
    "        out = []\n",
    "        for doc, meta in zip(docs, metas):\n",
    "            out.append({\n",
    "                \"plain_text\": doc,\n",
    "                \"structured_html\": meta[\"original_html\"],\n",
    "                \"metadata\": meta,\n",
    "            })\n",
    "        return out\n",
    "\n",
    "def create_dual_vector_db(html_content, max_size=1000):\n",
    "    chunks = chunkIt(html_content, max_size)\n",
    "    vector_db = DualRepresentationVectorDB()\n",
    "    vector_db.add_chunks(chunks)\n",
    "    return vector_db\n",
    "\n",
    "def llm(sys, prompt):\n",
    "    pr = {\n",
    "        \"model\": \"gemma3:4b\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": (\n",
    "                    sys\n",
    "                )\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"temperature\": 0,\n",
    "        \"max_tokens\": 250\n",
    "    }\n",
    "    resp = requests.post(\"http://localhost:11434/v1/chat/completions\", json=pr)\n",
    "\n",
    "    if resp.ok:\n",
    "        return resp.json()[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "    else:\n",
    "        return \"Could not connect to LLM\"\n",
    "\n",
    "def question_rewrite(ques):\n",
    "    q = \"You are a smart query rewriter. \"\\\n",
    "        \"Take the userâ€™s input and output only a concise, \"\\\n",
    "        \"self-contained question that could be asked directly to an API or assistant. \"\\\n",
    "        \"Do not ask for more information, just rewrite.\"\n",
    "    return llm(q, ques)\n",
    "\n",
    "def rag_pipeline(html_content: str, raw_query: str) -> str:\n",
    "    rewritten = question_rewrite(raw_query)\n",
    "\n",
    "    vector_db = create_dual_vector_db(html_content)\n",
    "    results = vector_db.search_with_html_context(rewritten, k=5)\n",
    "\n",
    "    context = \"\\n\\n\".join([r['structured_html'] for r in results])\n",
    "    user_prompt = f\"Context:\\n{context}\\n\\nQuestion: {rewritten}\"\n",
    "\n",
    "    ans = llm(\n",
    "        \"You are an assistant. Answer based on the provided context. \"\n",
    "        \"If unable to answer, say you cannot find the information.\",\n",
    "        user_prompt\n",
    "    )\n",
    "\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f593dcc5-191d-4e9c-ad10-03042e50a463",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, jsonify, request\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/')\n",
    "def home():\n",
    "    return \"ðŸš€ Flask server is running on port 5000!\"\n",
    "\n",
    "@app.route('/health/<name>')\n",
    "def health(name):\n",
    "    return f\"'Hello': {name}\"\n",
    "\n",
    "@app.errorhandler(404)\n",
    "def not_found(error):\n",
    "    return jsonify({\"error\": \"Not found\"}), 404\n",
    "\n",
    "# the only endpoint used.\n",
    "@app.route('/prompt', methods=['POST'])\n",
    "def generateOutput():\n",
    "    data = request.get_json()\n",
    "\n",
    "    if data is None:\n",
    "        return jsonify({\"error\" : \"Missing arguments for the POST request\"})\n",
    "\n",
    "    prompt = data.get(\"prompt\")\n",
    "    context = data.get(\"context\")\n",
    "\n",
    "    result = rag_pipeline(context, prompt)\n",
    "\n",
    "    print(result)\n",
    "\n",
    "    return jsonify({\n",
    "        \"success\": True,\n",
    "        \"response\": result\n",
    "    })\n",
    "\n",
    "app.run(host='0.0.0.0', port=5000, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2f7816-9475-4314-856d-7fac4e3ea2c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
